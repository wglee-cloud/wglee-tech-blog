---
description: 2022. 5. 21. 22:13
---

# 03. Rados Block Device (RBD) ì„¤ì¹˜í•˜ê¸°

cephì„ openstackì˜ cinderì™€ ê°™ì€ block stroage ì„œë¹„ìŠ¤ì˜ ë°±ì—”ë“œë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” RBDë¥¼ êµ¬ì„±í•´ì•¼ í•œë‹¤.\
RBD(Rados Block Device)ëŠ” Ceph Block Deviceë¼ê³ ë„ ì•ŒëŸ¬ì ¸ ìˆë‹¤.\
[https://docs.ceph.com/en/latest/rbd/rbd-openstack/](https://docs.ceph.com/en/latest/rbd/rbd-openstack/)

ì‘ì—…í•˜ê¸°ì— ì•ì„œ ì´ì „ì— êµ¬ì„±í•œ ceph í´ëŸ¬ìŠ¤í„°ì˜ ìƒíƒœê°€ HEALTH\_\_OKì¸ ê²ƒì„ í™•ì¸í•˜ê³  ì‹œì‘í•œë‹¤.\
[https://greencloud33.tistory.com/45](https://greencloud33.tistory.com/45) ì‘ì—… í›„ë¡œ pool ë° disk ìƒíƒœëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

```
root@deploy:/home/ceph-cluster# ceph osd lspools
1 device_health_metrics
2 .rgw.root
3 default.rgw.log
4 default.rgw.control
5 default.rgw.meta


root@deploy:/home/ceph-cluster# ceph df
--- RAW STORAGE ---
CLASS  SIZE     AVAIL    USED     RAW USED  %RAW USED
hdd    838 GiB  828 GiB  956 MiB   9.9 GiB       1.19
TOTAL  838 GiB  828 GiB  956 MiB   9.9 GiB       1.19

--- POOLS ---
POOL                   ID  PGS  STORED   OBJECTS  USED     %USED  MAX AVAIL
device_health_metrics   1    1      0 B        0      0 B      0    262 GiB
.rgw.root               2   32  1.3 KiB        4  768 KiB      0    262 GiB
default.rgw.log         3   32  3.4 KiB      207    6 MiB      0    262 GiB
default.rgw.control     4   32      0 B        8      0 B      0    262 GiB
default.rgw.meta        5    8      0 B        0      0 B      0    262 GiB
```



## deploy ì„œë²„ì—ì„œ ceph cli ì‚¬ìš©í•˜ê¸°

ì‚¬ì‹¤ ì´ ê³¼ì •ì€ RBD ì„¤ì¹˜ì™€ ë¬´ê´€í•˜ê¸°ëŠ” í•œë° í¸ì˜ì„±ì„ ìœ„í•´ì„œ ì§„í–‰í•˜ì˜€ë‹¤.\
deploy ì„œë²„ì— ceph clientë¥¼ ì„¤ì¹˜í•˜ì—¬ ceph í´ëŸ¬ìŠ¤í„°ì˜ ë‚´ìš©ì„ clië¡œ ì œì–´, ì¡°íšŒí•  ìˆ˜ ìˆê²Œ í•œë‹¤.

```
root@deploy:/home/ceph-cluster# ceph-deploy install ceph-client
```

íŒ¨í‚¤ì§€ë§Œ ì„¤ì¹˜í•˜ê³  deploy ì„œë²„ì—ì„œ ceph -së¥¼ í–ˆì„ ë•Œ ì˜¤ë¥˜ê°€ ë‚œë‹¤.\
ceph admin ê³„ì •ìœ¼ë¡œ ceph clusterì— ì¸ì¦ì„ ëª»ë°›ì•„ì„œ ê·¸ëŸ° ê²ƒìœ¼ë¡œ, ì—ëŸ¬ì— ë‚˜íƒ€ë‚˜ëŠ” /etc/ceph í•˜ìœ„ ê²½ë¡œì—\
ceph.admin.keyringì„ ë³µì‚¬í•´ ì¤€ë‹¤.

```
root@deploy:/home/ceph-cluster# ceph -s
2022-03-01T17:55:30.576+0900 7fa8edfb6700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory
2022-03-01T17:55:30.576+0900 7fa8edfb6700 -1 AuthRegistry(0x7fa8e80590e0) no keyring found at /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,, disabling cephx
2022-03-01T17:55:30.576+0900 7fa8ecd54700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory
2022-03-01T17:55:30.576+0900 7fa8ecd54700 -1 AuthRegistry(0x7fa8e805b698) no keyring found at /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,, disabling cephx
2022-03-01T17:55:30.576+0900 7fa8ecd54700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory
2022-03-01T17:55:30.576+0900 7fa8ecd54700 -1 AuthRegistry(0x7fa8ecd53130) no keyring found at /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,, disabling cephx
[errno 2] RADOS object not found (error connecting to the cluster)

root@deploy:/home/ceph-cluster# cp -a ceph.client.admin.keyring /etc/ceph
```

ì´ì œëŠ” deploy ì„œë²„ì—ì„œë„ ceph í´ëŸ¬ìŠ¤í„° ì •ë³´ë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆë‹¤.

```
root@deploy:/home/ceph-cluster# ceph -s
  cluster:
    id:     4ec23dde-416c-4a0b-8c6d-6d10a960b090
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum wglee-ceph-001,wglee-ceph-002,wglee-ceph-003 (age 4d)
    mgr: wglee-ceph-001(active, since 4d), standbys: wglee-ceph-002, wglee-ceph-003
    osd: 9 osds: 9 up (since 4d), 9 in (since 4d)
    rgw: 1 daemon active (wglee-ceph-001)

  task status:

  data:
    pools:   5 pools, 105 pgs
    objects: 187 objects, 4.7 KiB
    usage:   9.2 GiB used, 829 GiB / 838 GiB avail
    pgs:     105 active+clean
```





## ceph.conf ì„¤ì •í•˜ê¸°

pool ìƒì„± ì „ì— ceph.confì— osd default ê°’ ë° cache ì„¤ì •ì„ í•˜ì˜€ë‹¤.\
ceph.conf ìƒ˜í”Œì— ìˆëŠ” ì£¼ì„ì„ ì½ì–´ë³´ë‹ˆ osd pool default pg num ê°™ì€ ê²½ìš°ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•˜ëŠ”ê±¸ ê¶Œì¥í•œë‹¤ê³  í•œë‹¤.

```
(OSD ê°œìˆ˜ * 100 ) / replica ìˆ˜ì¹˜
```

ì € ê³„ì‚°ì— ì™„ì „ ë˜‘ê°™ì´ í•œê±´ ì•„ë‹ˆì§€ë§Œ ê·¼ì‚¬ì¹˜ë¡œ 320 ìœ¼ë¡œ ì„¤ì •í–ˆë‹¤.\
320ì€ PG calculatorë¡œ ê³„ì‚°í–ˆì„ ë•Œ ë‚˜ì˜¨ ê°’ì´ì—ˆë‹¤.

```
###config for ceph osds
osd pool default size = 3
osd pool default min size  = 2
osd pool default pg num = 320
osd pool default pgp num = 320

[client]
rbd cache = true
rbd cache size = 33554432 # (32MiB)
rbd cache max dirty = 20971520 # (20MiB)
rbd cache target dirty = 10485760 # (10MiB)
rbd cache max dirty age = 1.0
rbd cache writethrough until flush = true
```





## PG autoscaler Disable

PGë¥¼ ìë™ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ ë¶„ë°°í•˜ëŠ” ì˜µì…˜ì´ ì¼œì ¸ ìˆì—ˆë‹¤.\
ë‚˜ëŠ” pool ë³„ë¡œ ì‚¬ìš©ë¥ ì„ ê³ ë ¤í•´ì„œ ë¶„ë°°í•˜ê³  ì‹¶ì–´ì„œ ì´ë¥¼ disable í–ˆë‹¤.

```
root@deploy:/home/ceph-cluster# ceph osd pool set device_health_metrics pg_autoscale_mode off
set pool 1 pg_autoscale_mode to off
root@deploy:/home/ceph-cluster# ceph osd pool set  .rgw.root  pg_autoscale_mode off
set pool 2 pg_autoscale_mode to off
root@deploy:/home/ceph-cluster# ceph osd pool set  default.rgw.log  pg_autoscale_mode off
set pool 3 pg_autoscale_mode to off
root@deploy:/home/ceph-cluster# ceph osd pool set  default.rgw.control  pg_autoscale_mode off
set pool 4 pg_autoscale_mode to off
root@deploy:/home/ceph-cluster# ceph osd pool set  default.rgw.meta  pg_autoscale_mode off
set pool 5 pg_autoscale_mode to off
```

ì•ìœ¼ë¡œ ìƒì„±ë  pool ì— ëŒ€í•´ì„œë„ í•´ë‹¹ ì˜µì…˜ì„ ë„ê³  ì‹¶ìœ¼ë©´ ë‹¤ìŒê³¼ ê°™ì´ í•˜ë©´ ëœë‹¤.

```
root@deploy:/home/ceph-cluster# ceph config set global osd_pool_default_pg_autoscale_mode off

root@deploy:/home/ceph-cluster# ceph config dump
WHO     MASK  LEVEL     OPTION                                 VALUE  RO
global        advanced  osd_pool_default_pg_autoscale_mode     off
  mon         advanced  auth_allow_insecure_global_id_reclaim  false
  mon         advanced  mon_allow_pool_delete                  false
```

ëª¨ë‘ off ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

```
root@deploy:/home/ceph-cluster# ceph osd pool autoscale-status
POOL                     SIZE  TARGET SIZE  RATE  RAW CAPACITY   RATIO  TARGET RATIO  EFFECTIVE RATIO  BIAS  PG_NUM  NEW PG_NUM  AUTOSCALE
device_health_metrics      0                 3.0        838.1G  0.0000                                  1.0       1              off
.rgw.root               1289                 3.0        838.1G  0.0000                                  1.0      16              off
default.rgw.log         3520                 3.0        838.1G  0.0000                                  1.0      16              off
default.rgw.control        0                 3.0        838.1G  0.0000                                  1.0      16              off
default.rgw.meta           0                 3.0        838.1G  0.0000                                  4.0      16              off
```





## RBD pool ìƒì„±í•˜ê¸°

[https://docs.ceph.com/en/nautilus/rados/operations/pools/#create-a-pool](https://docs.ceph.com/en/nautilus/rados/operations/pools/#create-a-pool)\
ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ RBD ì„¤ì¹˜ë¥¼ ì‹œì‘í•œë‹¤. ë‹¤ìŒ poolì„ ìƒì„±í•˜ê³ , pg\_numì„ ìˆ˜ë™ìœ¼ë¡œ ì§€ì •í•´ ì£¼ë ¤ê³  í•œë‹¤.\
ê° poolì´ í•˜ëŠ” ì—­í• ì€ ë‹¤ìŒê³¼ ê°™ë‹¤\
\- **volume pool** : openstack cinderì™€ ì—°ë™. ë¸”ë¡ìŠ¤í† ë¦¬ì§€ ë° ë¸”ë¡ìŠ¤í† ë¦¬ì§€ ìŠ¤ëƒ…ìƒ·ì„ ì €ì¥\
\- **images pool** : openstack glanceì™€ ì—°ë™. VM ìƒì„±ì„ ìœ„í•œ OS imageì— í•´ë‹¹í•œë‹¤.\
&#x20;  !! ì£¼ì˜ì‚¬í•­ ) ceph ì´ìš©í•´ì„œ VMì˜ OS image ë§Œë“¤ë•ŒëŠ” QCOW2 ë³´ë‹¤ëŠ” raw íƒ€ì…ì„ ì‚¬ìš©ì´ ê¶Œì¥ëœë‹¤.\
\- **vms pool** : openstack VMì˜ ë¶€íŒ… ë””ìŠ¤í¬ë¥¼ ì €ì¥í•œë‹¤. Havana ë²„ì „ë¶€í„°ëŠ” ê°€ìƒì„œë²„ ë¶€íŒ…ë””ìŠ¤í¬ê°€ cinderë¥¼ íƒ€ì§€ ì•Šê³     ë°”ë¡œ cephê³¼ í†µì‹ í•˜ì—¬ ìƒì„±ëœë‹¤. ìœ ì§€ë³´ìˆ˜ ì‘ì—…ì—ì„œ ë²ˆê±°ë¡œì›€ì„ ëœê³  live-migration ì—ë„ ì´ì ì´ ìˆë‹¤ê³  í•œë‹¤.\
\- **backups pool** : ë¸”ë¡ ìŠ¤í† ë¦¬ì§€ë¥¼ ë°±ì—…í•  ìˆ˜ ìˆëŠ” cinderì˜ ê¸°ëŠ¥ì´ ìˆê³ , ë°±ì—… ë‚´ìš©ì„ ì €ì¥í•˜ëŠ” poolë¡œ ë³´ì¸ë‹¤.\
&#x20; ë‚˜ëŠ” ì•„ì§ clnder backup êµ¬í˜„ê¹Œì§€ëŠ” ìƒê°ì„ ì•ˆí•˜ê³  ìˆì§€ë§Œ ì¼ë‹¨ poolì€ ìƒì„±í–ˆë‹¤.

```
root@deploy:/home/ceph-cluster# ceph osd pool create volumes
pool 'volumes' created
root@deploy:/home/ceph-cluster# ceph osd pool create images
pool 'images' created
root@deploy:/home/ceph-cluster# ceph osd pool create backups
pool 'backups' created
root@deploy:/home/ceph-cluster# ceph osd pool create vms
pool 'vms' created
```



## pg\_num ì ìš©

ì‚¬ì‹¤ ë‚´ê°€ ë§ê²Œ í•œ ê²ƒì¸ì§€ ì˜ ëª¨ë¥´ê² ë‹¤..\
[https://access.redhat.com/labs/cephpgc/](https://access.redhat.com/labs/cephpgc/) ì—¬ê¸°ì„œ osd 10ê°œ, replica 3ìœ¼ë¡œ ì„¤ì •í•˜ê³  í•„ìš”í•œ poolë“¤ì„ ì…ë ¥í•´ì„œ ê³„ì‚°í•´ ë³´ì•˜ë‹¤.

<div align="left">

<figure><img src="https://blog.kakaocdn.net/dn/H85X4/btrCMXrZehj/B5ZwDBBo2wMf0AslOX3gEK/img.png" alt=""><figcaption></figcaption></figure>

</div>

\
ì´ ì‚¬ì§„ì—ì„œ ê³„ì‚°ëœ ìˆ˜ì¹˜ë¡œ ì ìš©í–ˆë‹¤.&#x20;

```
root@deploy:/home/ceph-cluster# ceph osd pool set device_health_metrics pg_num 16
root@deploy:/home/ceph-cluster# ceph osd pool set device_health_metrics pgp_num 16
set pool 1 pgp_num to 16
root@deploy:/home/ceph-cluster# ceph osd pool get .rgw.root pgp_num
pgp_num: 16
root@deploy:/home/ceph-cluster# ceph osd pool get .rgw.root pg_num
pg_num: 16
root@deploy:/home/ceph-cluster# ceph osd pool set .rgw.root pg_num 16
root@deploy:/home/ceph-cluster# ceph osd pool set .rgw.root pgp_num 16
set pool 2 pgp_num to 16
root@deploy:/home/ceph-cluster# ceph osd pool set default.rgw.log pg_num 16
root@deploy:/home/ceph-cluster# ceph osd pool set default.rgw.log pgp_num 16
set pool 3 pgp_num to 16
root@deploy:/home/ceph-cluster# ceph osd pool set default.rgw.control pg_num 16
root@deploy:/home/ceph-cluster# ceph osd pool set default.rgw.control pgp_num 16
set pool 4 pgp_num to 16
root@deploy:/home/ceph-cluster# ceph osd pool set default.rgw.meta pg_num 16
root@deploy:/home/ceph-cluster# ceph osd pool set default.rgw.meta pgp_num 16
set pool 5 pgp_num to 16
root@deploy:/home/ceph-cluster# ceph osd pool set backups pg_num 32
root@deploy:/home/ceph-cluster# ceph osd pool set backups pgp_num 32
set pool 12 pgp_num to 32
root@deploy:/home/ceph-cluster# ceph osd pool set volumes pg_num 128
set pool 10 pg_num to 128
root@deploy:/home/ceph-cluster# ceph osd pool set volumes pgp_num 128
set pool 10 pgp_num to 128
root@deploy:/home/ceph-cluster# ceph osd pool set images pg_num 16
set pool 11 pg_num to 16
root@deploy:/home/ceph-cluster# ceph osd pool set images pgp_num 16
set pool 11 pgp_num to 16
root@deploy:/home/ceph-cluster# ceph osd pool set vms pg_num 64
set pool 13 pg_num to 64
root@deploy:/home/ceph-cluster# ceph osd pool set vms pgp_num 64
set pool 13 pgp_num to 64
```

ceph statusë¥¼ watch ëª…ë ¹ìœ¼ë¡œ í™•ì¸í•˜ì—¬ ëª¨ë“  PGê°€ active+clean ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦°ë‹¤.\
320 ê°œì˜ PGê°€ ëª¨ë‘ active+clean ë˜ì—ˆë‹¤.

```
Every 2.0s: ceph -s                                                                                    deploy: Sat May 21 17:36:31 2022

  cluster:
    id:     4ec23dde-416c-4a0b-8c6d-6d10a960b090
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum wglee-ceph-001,wglee-ceph-002,wglee-ceph-003 (age 3h)
    mgr: wglee-ceph-002(active, since 3h), standbys: wglee-ceph-001, wglee-ceph-003
    osd: 9 osds: 9 up (since 3h), 9 in (since 2M)
    rgw: 1 daemon active (wglee-ceph-001)

  task status:

  data:
    pools:   9 pools, 320 pgs
    objects: 219 objects, 4.7 KiB
    usage:   10 GiB used, 828 GiB / 838 GiB avail
    pgs:     320 active+clean
```

ceph df ê²°ê³¼. PGê°€ ë‚´ê°€ ì„¤ì •í•œ ëŒ€ë¡œ ë¶„ë°°ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

```

root@deploy:/home/ceph-cluster# ceph df
--- RAW STORAGE ---
CLASS  SIZE     AVAIL    USED     RAW USED  %RAW USED
hdd    838 GiB  828 GiB  1.1 GiB    10 GiB       1.20
TOTAL  838 GiB  828 GiB  1.1 GiB    10 GiB       1.20

--- POOLS ---
POOL                   ID  PGS  STORED   OBJECTS  USED     %USED  MAX AVAIL
device_health_metrics   1   16      0 B        0      0 B      0    262 GiB
.rgw.root               2   16  1.3 KiB        4  768 KiB      0    262 GiB
default.rgw.log         3   16  3.4 KiB      207    6 MiB      0    262 GiB
default.rgw.control     4   16      0 B        8      0 B      0    262 GiB
default.rgw.meta        5   16      0 B        0      0 B      0    262 GiB
volumes                10  128      0 B        0      0 B      0    262 GiB
images                 11   16      0 B        0      0 B      0    262 GiB
backups                12   32      0 B        0      0 B      0    262 GiB
vms                    13   64      0 B        0      0 B      0    262 GiB
```



## RBD init

poolì„ ìƒì„±í•˜ë©´ ì–´ë–¤ applicationì—ì„œ ì‚¬ìš©í•  ê²ƒì¸ì§€ë¥¼ ì§€ì •í•´ì•¼ í•œë‹¤. rbd block device ê°™ì€ ê²½ìš°ëŠ” ë‹¤ìŒê³¼ ê°™ì´ init í•˜ì—¬ ì´ë¥¼ ìˆ˜í–‰í•œë‹¤.

```
root@deploy:/home/ceph-cluster# rbd pool init volumes
root@deploy:/home/ceph-cluster# rbd pool init images
root@deploy:/home/ceph-cluster# rbd pool init backups
root@deploy:/home/ceph-cluster# rbd pool init vms
```



## RBD object ìƒì„± í…ŒìŠ¤íŠ¸

ì´ì œ pool ê¹Œì§€ ì„¤ì •ì´ ì™„ë£Œ ë˜ì—ˆìœ¼ë‹ˆ ë¦¬ì†ŒìŠ¤ê°€ ì˜ ìƒì„±ë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸ í•´ ë³¸ë‹¤.\
\--sizeì˜ ì¸ìë¡œ ë“¤ì–´ê°€ëŠ” ê°’ì˜ ë‹¨ìœ„ëŠ” MiBì´ë‹¤. ë‚˜ëŠ” 1MiB ë¥¼ í•˜ë‚˜ ìƒì„±í•´ ë³´ì•˜ë‹¤.

```
root@deploy:/home/ceph-cluster# rbd create --size 1 volumes/wglee-test
```

ë‹¤ìŒê³¼ ê°™ì´ wglee-test ë¼ëŠ” objectê°€ volumes poolì— ìƒì„±ë˜ì—ˆë‹¤.\
volumes poolì˜ STORED ë°ì´í„°ë„ 70Bë¡œ ì¦ê°€í–ˆë‹¤.

```
root@deploy:/home/ceph-cluster# rbd ls volumes
wglee-test

root@deploy:/home/ceph-cluster# rbd info volumes/wglee-test
rbd image 'wglee-test':
        size 1 MiB in 1 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 6a5831cfe8f3
        block_name_prefix: rbd_data.6a5831cfe8f3
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features:
        flags:
        create_timestamp: Sat May 21 17:50:32 2022
        access_timestamp: Sat May 21 17:50:32 2022
        modify_timestamp: Sat May 21 17:50:32 2022

root@deploy:/home/ceph-cluster# ceph df
...
--- POOLS ---
POOL                   ID  PGS  STORED   OBJECTS  USED     %USED  MAX AVAIL
...
volumes                10  128     70 B        5  576 KiB      0    262 GiB
images                 11   16     19 B        1  192 KiB      0    262 GiB
backups                12   32     19 B        1  192 KiB      0    262 GiB
vms                    13   64     19 B        1  192 KiB      0    262 GiB
```

rbd resizeë¡œ í¬ê¸°ë¥¼ ë³€ê²½í•´ ë³¸ë‹¤.

```
root@deploy:/home/ceph-cluster# rbd resize --size 5 volumes/wglee-test
Resizing image: 100% complete...done.

root@deploy:/home/ceph-cluster# rbd info volumes/wglee-test
rbd image 'wglee-test':
        size 5 MiB in 2 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: 6a5831cfe8f3
        block_name_prefix: rbd_data.6a5831cfe8f3
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features:
        flags:
        create_timestamp: Sat May 21 17:50:32 2022
        access_timestamp: Sat May 21 17:50:32 2022
        modify_timestamp: Sat May 21 17:50:32 2022
```

í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ìƒì„±í•œê±° ì‚­ì œí•˜ê¸°\
ì‚­ì œ í•œ í›„ì— df í–ˆì„ ë•Œ volumes pool ìš©ëŸ‰ë„ ì¤„ì–´ë“¤ì—ˆë‹¤. ì˜ ë˜ëŠ” ë“¯ í•˜ë‹¤! ğŸ˜‰ğŸ˜ƒ

```
root@deploy:/home/ceph-cluster# rbd rm volumes/wglee-test
Removing image: 100% complete...done.

root@deploy:/home/ceph-cluster# ceph df
--- RAW STORAGE ---
CLASS  SIZE     AVAIL    USED     RAW USED  %RAW USED
hdd    838 GiB  828 GiB  1.1 GiB    10 GiB       1.21
TOTAL  838 GiB  828 GiB  1.1 GiB    10 GiB       1.21

--- POOLS ---
POOL                   ID  PGS  STORED   OBJECTS  USED     %USED  MAX AVAIL
...
volumes                10  128     19 B        3  192 KiB      0    262 GiB
...
```
